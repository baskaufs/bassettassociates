{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omeka_upload_data.py - a script to manage CSV data creation and upload with Omeka Classic installations\n",
    "\n",
    "# (c) 2023 Steven J Baskauf. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# NOTE: The AWS access keys for writing to the S3 bucket must be stored in the ~/.aws/credentials file before running this script.\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import boto3 # AWS SDK for Python\n",
    "\n",
    "# -------------------\n",
    "# global variables\n",
    "# -------------------\n",
    "DATA_PATH = '../data/'\n",
    "PYRAMIDAL_TIFFS_DIRECTORY_PATH = '/Users/baskausj/pyramidal_tiffs/'\n",
    "UPLOAD_FILE_BASE_DIRECTORY_PATH = '/Users/baskausj/Downloads/bassett_raw_images/'\n",
    "DIRECTORY_SUBPATH = 'zoo/kcz/master/'\n",
    "S3_BUCKET = 'bassettassociates'\n",
    "\n",
    "FORMAT_MAP = {\n",
    "    'jpg': 'image/jpeg',\n",
    "    'png': 'image/png',\n",
    "    'tif': 'image/tiff',\n",
    "    'gif': 'image/gif'\n",
    "}\n",
    "\n",
    "ORIGINAL_FORMAT_MAP = {\n",
    "    'ph': 'photo',\n",
    "    'sk': 'sketch',\n",
    "    'pl': 'plan',\n",
    "    'mo': 'model'\n",
    "}\n",
    "\n",
    "CREATOR_MAP = {\n",
    "    'ph': 'Bassett Associates',\n",
    "    'sk': 'James H. Bassett',\n",
    "    'pl': 'Bassett Associates',\n",
    "    'mo': 'Bassett Associates'\n",
    "} \n",
    "\n",
    "TAGS_MAP = {\n",
    "    'zoo': 'zoo',\n",
    "    'cmp': 'campus',\n",
    "    'cbd': 'downtown',\n",
    "    'mrf': 'Muirfield',\n",
    "    'pvt': 'private estate',\n",
    "    'kcz': 'Kansas City',\n",
    "    'onu': 'ONU',\n",
    "    'blu': 'Bluffton',\n",
    "    'osm': 'OSU-Marion',\n",
    "    'osl': 'OSU-Lima',\n",
    "    'bgu': 'BGSU',\n",
    "    'fin': 'Findlay',\n",
    "    'lim': 'Lima',\n",
    "    'bfn': 'Bellfontaine',\n",
    "    'tif': 'Tiffin',\n",
    "    'mns': 'Mansfield'\n",
    "}\n",
    "\n",
    "''' Enable this after converting to a command line script\n",
    "# If the DIRECTORY_SUBPATH is provided at run time, extract the subpath from the command line arguments\n",
    "if len(sys.argv) > 1:\n",
    "    DIRECTORY_SUBPATH = sys.argv[1]\n",
    "'''\n",
    "\n",
    "# -------------------\n",
    "# Functions\n",
    "# -------------------\n",
    "\n",
    "def move_pyramidal_tiffs_to_upload_subdirectory(directory_subpath: str, upload_file_base_directory_path: str, pyramidal_tiffs_directory_path: str) -> None:\n",
    "    # Find out if the upload directory exists and create it if not\n",
    "    upload_directory_path = upload_file_base_directory_path + directory_subpath\n",
    "    print('upload_directory_path:', upload_directory_path)\n",
    "    if not os.path.exists(upload_directory_path):\n",
    "        print('directory does not exist, creating it')\n",
    "        os.makedirs(upload_directory_path)\n",
    "    else:\n",
    "        print('directory exists')\n",
    "\n",
    "    # Invoke linux command to move the pyramidal tiffs to the upload directory\n",
    "    # Note: the -n option prevents overwriting existing files\n",
    "    os.system('mv -n ' + pyramidal_tiffs_directory_path + '*.tif ' + upload_file_base_directory_path + directory_subpath)\n",
    "\n",
    "\n",
    "def aws_s3_upload(s3_bucket: str, directory_subpath: str, upload_file_base_directory_path: str) -> None:\n",
    "    s3 = boto3.client('s3')\n",
    "    local_path_root_path = upload_file_base_directory_path + directory_subpath\n",
    "\n",
    "    # Loop through all files in the local directory\n",
    "    for local_filename in os.listdir(local_path_root_path):\n",
    "        # Skip the .DS_Store file (if on a Mac)\n",
    "        if local_filename == '.DS_Store':\n",
    "            continue\n",
    "        s3_iiif_key = directory_subpath + local_filename\n",
    "        print('Uploading to s3:', local_filename)\n",
    "        s3.upload_file(local_path_root_path + local_filename, s3_bucket, s3_iiif_key)\n",
    "        print('Done uploading to s3:', local_filename)\n",
    "        print()\n",
    "\n",
    "def generate_metadata_csv_for_omeka_upload(s3_bucket: str, directory_subpath: str, upload_file_base_directory_path: str, data_path: str) -> None:\n",
    "    # Read the empty CSV file into a dataframe to get the file headers, no NA values, read empty cells as empty strings\n",
    "    upload_df = pd.read_csv(data_path + 'upload_headers.csv', na_filter=False, dtype=str)\n",
    "    # Set the \"Dublin Core:Identifier\" column as the index\n",
    "    upload_df = upload_df.set_index('Dublin Core:Identifier')\n",
    "\n",
    "    # Get the list of files in the upload directory\n",
    "    upload_file_directory_path = upload_file_base_directory_path + directory_subpath\n",
    "    upload_file_list = os.listdir(upload_file_directory_path)\n",
    "\n",
    "    # Set the base file URL for the S3 bucket\n",
    "    upload_file_base_url = 'https://' + s3_bucket + '.s3.amazonaws.com/' + directory_subpath\n",
    "\n",
    "    # Remove the .DS_Store file from the list (if on a Mac)\n",
    "    if '.DS_Store' in upload_file_list:\n",
    "        upload_file_list.remove('.DS_Store')\n",
    "\n",
    "    # Loop through the files in the upload directory\n",
    "    for file_name in upload_file_list:\n",
    "        # Get the file name without the extension to use as the image_id\n",
    "        image_id = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # Create a series (string datatype) for the row to be added to the dataframe, using the image_id as the index\n",
    "        row_series = pd.Series(index=upload_df.columns, name=image_id, dtype=str)\n",
    "\n",
    "        # Set the values of constant columns\n",
    "        row_series['Dublin Core:Type'] = 'StillImage'\n",
    "        row_series['Dublin Core:Rights'] = 'Available under a Creative Commons Attribution 4.0 International (CC BY 4.0) license'\n",
    "        row_series['Dublin Core:Source'] = 'Bassett Associates files'\n",
    "        row_series['Dublin Core:Publisher'] = 'James H. Bassett'\n",
    "        \n",
    "        # Create the upload URL from the base URL and the file name, then add it to the series as the upload_url value\n",
    "        upload_url = upload_file_base_url + file_name\n",
    "        row_series['upload_url'] = upload_url\n",
    "\n",
    "        # Extract the original format type from the image_id, look it up in the map, and assign to the series. \n",
    "        # Example: zoo_kcz_chimp_ph_00, extract the next to last part (\"ph\"), look up \"photograph\".\n",
    "        original_format_code = image_id.split('_')[-2]\n",
    "        original_format = ORIGINAL_FORMAT_MAP[original_format_code]\n",
    "        row_series['Item Type Metadata:Original Format'] = original_format\n",
    "\n",
    "        # Extract the image dimensions from the file, construct the dimension string, and assign to the series\n",
    "        image = Image.open(upload_file_directory_path + file_name)\n",
    "        image_width, image_height = image.size\n",
    "        dimensions = str(image_width) + 'x' + str(image_height)\n",
    "        row_series['Item Type Metadata:Physical Dimensions'] = dimensions\n",
    "\n",
    "        # Extract the file extension from the file name, look it up in the map, and assign to the series\n",
    "        file_extension = file_name.split('.')[-1]\n",
    "        file_format = FORMAT_MAP[file_extension]\n",
    "        row_series['Dublin Core:Format'] = file_format\n",
    "\n",
    "        # For original_format_codes \"sk\" and \"pl\", assign \"en\" as the language.\n",
    "        if original_format_code in ['sk', 'pl']:\n",
    "            row_series['Dublin Core:Language'] = 'en'\n",
    "        else:\n",
    "            row_series['Dublin Core:Language'] = ''\n",
    "\n",
    "        # Set Title, Description, and Date values to empty strings\n",
    "        row_series['Dublin Core:Title'] = ''\n",
    "        row_series['Dublin Core:Description'] = ''\n",
    "        row_series['Dublin Core:Date'] = ''\n",
    "\n",
    "        # Set Creator value based on original_format_code\n",
    "        row_series['Dublin Core:Creator'] = CREATOR_MAP[original_format_code]\n",
    "\n",
    "        # Set tags base on first two parts of the identifier\n",
    "        tags = image_id.split('_')[:2]\n",
    "        # Construct comma-separated list of tags\n",
    "        tag_list = []\n",
    "        for tag in tags:\n",
    "            if tag in TAGS_MAP:\n",
    "                tag_list.append(TAGS_MAP[tag])\n",
    "        tag_string = ','.join(tag_list)\n",
    "        row_series['tags'] = tag_string\n",
    "\n",
    "        # Add the series to the dataframe\n",
    "        upload_df = upload_df.append(row_series)\n",
    "\n",
    "    # Write the dataframe to a CSV file\n",
    "    upload_df.to_csv(data_path + 'upload.csv', index=True, index_label='Dublin Core:Identifier')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving pyramidal TIFFs to upload directory...\n",
      "upload_directory_path: /Users/baskausj/Downloads/bassett_raw_images/zoo/kcz/master/\n",
      "directory does not exist, creating it\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Main\n",
    "# -------------------\n",
    "\n",
    "print('moving pyramidal TIFFs to upload directory...')\n",
    "move_pyramidal_tiffs_to_upload_subdirectory(DIRECTORY_SUBPATH, UPLOAD_FILE_BASE_DIRECTORY_PATH, PYRAMIDAL_TIFFS_DIRECTORY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating metadata CSV for Omeka upload...\n"
     ]
    }
   ],
   "source": [
    "print('generating metadata CSV for Omeka upload...')\n",
    "generate_metadata_csv_for_omeka_upload(S3_BUCKET, DIRECTORY_SUBPATH, UPLOAD_FILE_BASE_DIRECTORY_PATH, DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading files to S3...\n",
      "Uploading to s3: zoo_kcz_master_pl_00.tif\n",
      "Done uploading to s3: zoo_kcz_master_pl_00.tif\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('uploading files to S3...')\n",
    "aws_s3_upload(S3_BUCKET, DIRECTORY_SUBPATH, UPLOAD_FILE_BASE_DIRECTORY_PATH)\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
